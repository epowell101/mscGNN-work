{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+eaDeu20lbDd9nT88BZi1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/epowell101/mscGNN-work/blob/main/EDA_and_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQSdS2xLLvhr",
        "outputId": "67004579-b96b-43f9-ca1d-67750ec818d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Required Libraries\n",
        "import pandas as pd\n",
        "import os\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 1: File Exploration\n",
        "# -------------------------\n",
        "# Define the exact path to your Parquet file in Google Drive\n",
        "exact_path = '/content/drive/My Drive/ETH data/tx_data_loki_gr15.parquet'\n",
        "\n",
        "# Load the Parquet file into a DataFrame\n",
        "df = pd.read_parquet(exact_path)\n",
        "\n",
        "# Remove rows where 'from_address' or 'to_address' is None\n",
        "df = df[df['from_address'].notna() & df['to_address'].notna()]\n",
        "\n",
        "# Randomly sample X% of the data (adjust the fraction as needed)\n",
        "sample_df = df.sample(frac=0.10)\n",
        "\n",
        "# Initialize a directed graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Loop through all rows in the DataFrame to add nodes and edges to the graph\n",
        "for idx, row in sample_df.iterrows():\n",
        "    G.add_edge(row['from_address'], row['to_address'],\n",
        "               block_timestamp=row['block_timestamp'],\n",
        "               eth_value=row['eth_value'],\n",
        "               gas_used=row['gas_used'],  # Added gas_used\n",
        "               tx_fee=row['tx_fee'])      # Added tx_fee\n",
        "\n",
        "# Updating node types based on their appearance in 'from_address' and 'to_address'\n",
        "# Here, the change is to use sample_df instead of df for node type assignment\n",
        "for node in G.nodes():\n",
        "    node_type = []\n",
        "    if node in sample_df['from_address'].values:  # Changed from df to sample_df\n",
        "        node_type.append('From')\n",
        "    if node in sample_df['to_address'].values:    # Changed from df to sample_df\n",
        "        node_type.append('To')\n",
        "    if node in sample_df['EOA'].values:           # Changed from df to sample_df\n",
        "        node_type.append('EOA')\n",
        "    G.nodes[node]['type'] = node_type\n",
        "\n",
        "# Initialize a dictionary to store the depth for each EOA\n",
        "depth_dict = {}\n",
        "\n",
        "# Define the maximum depth you're interested in\n",
        "max_depth = 10\n",
        "\n",
        "# Loop through all unique EOAs to calculate depth\n",
        "# The change here is to use sample_df instead of df for the unique EOAs list\n",
        "for eoa in sample_df['EOA'].unique():  # Changed from df to sample_df\n",
        "    visited = set()\n",
        "    to_explore = [(eoa, 0)]\n",
        "    while to_explore:\n",
        "        current_node, current_depth = to_explore.pop(0)\n",
        "        if current_node in visited or current_depth > max_depth:\n",
        "            continue\n",
        "        if current_node not in G:  # Added this check to handle missing nodes\n",
        "            continue\n",
        "        visited.add(current_node)\n",
        "        neighbors = list(G.successors(current_node))\n",
        "        to_explore.extend((neighbor, current_depth + 1) for neighbor in neighbors)\n",
        "    depth_dict[eoa] = len(visited)\n",
        "\n",
        "# Count or list EOAs connected to other EOAs\n",
        "eoa_to_eoa_count = 0\n",
        "eoa_to_eoa_list = []\n",
        "\n",
        "# Use sample_df for the unique EOAs list\n",
        "for eoa in sample_df['EOA'].unique():  # Changed from df to sample_df\n",
        "    for neighbor in G.successors(eoa):\n",
        "        if 'EOA' in G.nodes[neighbor].get('type', []):  # Changed the condition to check for 'EOA' in the list\n",
        "            eoa_to_eoa_count += 1\n",
        "            eoa_to_eoa_list.append((eoa, neighbor))\n",
        "\n",
        "print(f\"Number of EOA to EOA connections: {eoa_to_eoa_count}\")\n",
        "print(f\"List of EOA to EOA connections: {eoa_to_eoa_list}\")\n",
        "\n",
        "# Convert depth dictionary to DataFrame for easier manipulation and plotting\n",
        "depth_df = pd.DataFrame(list(depth_dict.items()), columns=['EOA', 'Depth'])\n",
        "\n",
        "# Basic statistics on depth\n",
        "print(\"Depth statistics:\")\n",
        "print(depth_df['Depth'].describe())\n",
        "\n",
        "# Step 3: Basic EDA\n",
        "# -----------------\n",
        "# View first few rows\n",
        "print(\"First few rows of the DataFrame:\")\n",
        "print(df.head())\n",
        "\n",
        "# Number of unique EOA, From and To\n",
        "unique_eoas = df['EOA'].nunique()\n",
        "print(f\"Number of unique EOAs: {unique_eoas}\")\n",
        "\n",
        "unique_from=df['from_address'].nunique()\n",
        "print(f\"Number of unique from addresses: {unique_from}\")\n",
        "\n",
        "unique_to=df['to_address'].nunique()\n",
        "print(f\"Number of unique to addresses: {unique_to}\")\n",
        "\n",
        "# Summary statistics\n",
        "print(\"Summary statistics:\")\n",
        "print(df.describe())\n",
        "\n",
        "# Measure depth of transactions for each EOA\n",
        "depth_dict = {}  # Initialize a dictionary to store the depth for each EOA\n",
        "max_depth = 5  # Replace with the maximum depth you're interested in\n",
        "\n",
        "# Convert depth dictionary to DataFrame for easier manipulation and plotting\n",
        "depth_df = pd.DataFrame(list(depth_dict.items()), columns=['EOA', 'Depth'])\n",
        "\n",
        "# Basic statistics on depth\n",
        "print(\"Depth statistics:\")\n",
        "print(depth_df['Depth'].describe())\n",
        "\n",
        "# Convert depth dictionary to DataFrame for easier manipulation and plotting\n",
        "depth_df = pd.DataFrame(list(depth_dict.items()), columns=['EOA', 'Depth'])\n",
        "\n",
        "# Basic statistics on depth\n",
        "print(\"Depth statistics:\")\n",
        "print(depth_df['Depth'].describe())\n",
        "\n",
        "# Calculate centrality metrics for all nodes\n",
        "degree_centrality = nx.degree_centrality(G)\n",
        "closeness_centrality = nx.closeness_centrality(G)\n",
        "betweenness_centrality = nx.betweenness_centrality(G)\n",
        "# eigenvector_centrality = nx.eigenvector_centrality(G)\n",
        "\n",
        "# Create dictionaries to store centrality metrics for the two groups\n",
        "eoa_to_eoa_centrality = {}\n",
        "other_eoa_centrality = {}\n",
        "\n",
        "# Populate the dictionaries\n",
        "for eoa in sample_df['EOA'].unique():\n",
        "    if eoa in G:\n",
        "        centrality_metrics = {\n",
        "            'degree': degree_centrality.get(eoa, 0),\n",
        "            'closeness': closeness_centrality.get(eoa, 0),\n",
        "            'betweenness': betweenness_centrality.get(eoa, 0),\n",
        "        }\n",
        "\n",
        "        if eoa in [e[0] for e in eoa_to_eoa_list]:\n",
        "            eoa_to_eoa_centrality[eoa] = centrality_metrics\n",
        "        else:\n",
        "            other_eoa_centrality[eoa] = centrality_metrics\n",
        "\n",
        "# Create DataFrames for the two groups\n",
        "eoa_to_eoa_df = sample_df[sample_df['EOA'].isin([e[0] for e in eoa_to_eoa_list])]\n",
        "other_eoa_df = sample_df[~sample_df['EOA'].isin([e[0] for e in eoa_to_eoa_list])]\n",
        "\n",
        "# Calculate average transaction sizes and their variance\n",
        "avg_size_eoa_to_eoa = eoa_to_eoa_df['eth_value'].mean()\n",
        "var_size_eoa_to_eoa = eoa_to_eoa_df['eth_value'].var()\n",
        "\n",
        "avg_size_other_eoa = other_eoa_df['eth_value'].mean()\n",
        "var_size_other_eoa = other_eoa_df['eth_value'].var()\n",
        "\n",
        "# Calculate mean and standard deviation for each group\n",
        "eoa_to_eoa_mean = eoa_to_eoa_df.mean()\n",
        "eoa_to_eoa_std = eoa_to_eoa_df.std()\n",
        "\n",
        "other_eoa_mean = other_eoa_df.mean()\n",
        "other_eoa_std = other_eoa_df.std()\n",
        "\n",
        "# Make sure that all the arrays have the same length by taking only the common metrics\n",
        "common_metrics = set(eoa_to_eoa_mean.index) & set(eoa_to_eoa_std.index) & set(other_eoa_mean.index) & set(other_eoa_std.index)\n",
        "\n",
        "# Filter the Series objects to include only the common metrics\n",
        "eoa_to_eoa_mean = eoa_to_eoa_mean[common_metrics]\n",
        "eoa_to_eoa_std = eoa_to_eoa_std[common_metrics]\n",
        "other_eoa_mean = other_eoa_mean[common_metrics]\n",
        "other_eoa_std = other_eoa_std[common_metrics]\n",
        "\n",
        "# Convert dictionaries to DataFrames for easier manipulation\n",
        "eoa_to_eoa_centrality_df = pd.DataFrame.from_dict(eoa_to_eoa_centrality, orient='index')\n",
        "other_eoa_centrality_df = pd.DataFrame.from_dict(other_eoa_centrality, orient='index')\n",
        "\n",
        "# Create a summary DataFrame\n",
        "summary_df = pd.DataFrame({\n",
        "    'Metric': list(common_metrics),\n",
        "    'EOA_to_EOA_Mean': eoa_to_eoa_mean.values,\n",
        "    'EOA_to_EOA_Std': eoa_to_eoa_std.values,\n",
        "    'Other_EOA_Mean': other_eoa_mean.values,\n",
        "    'Other_EOA_Std': other_eoa_std.values\n",
        "})\n",
        "\n",
        "# Display the summary DataFrame\n",
        "print(summary_df)\n",
        "\n",
        "print(f\"EOA to EOA Avg Size: {avg_size_eoa_to_eoa}, Variance: {var_size_eoa_to_eoa}\")\n",
        "print(f\"Other EOA Avg Size: {avg_size_other_eoa}, Variance: {var_size_other_eoa}\")\n",
        "\n",
        "# Calculate mean and standard deviation for centrality metrics\n",
        "eoa_to_eoa_centrality_mean = eoa_to_eoa_centrality_df.mean()\n",
        "eoa_to_eoa_centrality_std = eoa_to_eoa_centrality_df.std()\n",
        "\n",
        "other_eoa_centrality_mean = other_eoa_centrality_df.mean()\n",
        "other_eoa_centrality_std = other_eoa_centrality_df.std()\n",
        "\n",
        "# Create a summary DataFrame for centrality metrics\n",
        "centrality_summary_df = pd.DataFrame({\n",
        "    'Centrality_Metric': list(eoa_to_eoa_centrality_mean.index),\n",
        "    'EOA_to_EOA_Mean': eoa_to_eoa_centrality_mean.values,\n",
        "    'EOA_to_EOA_Std': eoa_to_eoa_centrality_std.values,\n",
        "    'Other_EOA_Mean': other_eoa_centrality_mean.values,\n",
        "    'Other_EOA_Std': other_eoa_centrality_std.values\n",
        "})\n",
        "\n",
        "# Display the summary DataFrame for centrality metrics\n",
        "print(\"Summary statistics for centrality metrics:\")\n",
        "print(centrality_summary_df)\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Missing values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "def df_to_markdown(df):\n",
        "    fmt = ['---' for _ in range(len(df.columns))]\n",
        "    df_fmt = pd.DataFrame([fmt], columns=df.columns)\n",
        "    df_formatted = pd.concat([df_fmt, df])\n",
        "    return df_formatted.to_markdown(index=False)\n",
        "\n",
        "# Convert summary DataFrame to Markdown\n",
        "summary_markdown = df_to_markdown(summary_df)\n",
        "print(summary_markdown)\n",
        "\n",
        "# Convert centrality summary DataFrame to Markdown\n",
        "centrality_summary_markdown = df_to_markdown(centrality_summary_df)\n",
        "print(centrality_summary_markdown)\n",
        "\n",
        "\n",
        "# Data distribution (use histograms or boxplots)\n",
        "# sns.pairplot(df)\n",
        "# plt.show()\n",
        "\n",
        "# Step 4: Data Preprocessing\n",
        "# ---------------------------\n",
        "# Perform any required preprocessing steps here. This will be influenced by your script.\n",
        "# For example, if you want to bucketize a column:\n",
        "# df['amount_bucket'] = pd.cut(df['amount'], bins=[0, 1000, 10000, 100000, float('inf')])\n",
        "\n",
        "# Step 5: Save Processed Data\n",
        "# ---------------------------\n",
        "# Save the DataFrame as a new Parquet file\n",
        "processed_file_path = '/content/drive/My Drive/ETH data/Sybildata_first.parquet'\n",
        "df.to_parquet(processed_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a function for a prettier table\n",
        "\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def display_styled_dataframe(df):\n",
        "    \"\"\"\n",
        "    Display a DataFrame with formatting in Jupyter Notebook\n",
        "    \"\"\"\n",
        "    float_columns = df.select_dtypes(include=['float64']).columns\n",
        "    format_dict = {col: \"{:.2}\" for col in float_columns}\n",
        "    styled_df = df.style.format(format_dict)\n",
        "    display(styled_df)\n",
        "\n",
        "# Using the function\n",
        "display_styled_dataframe(summary_df) # summary\n",
        "display_styled_dataframe(centrality_summary_df) # centrality summary\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "KhU7Yp79tVbJ",
        "outputId": "0744e0ba-4ee0-475c-ee33-261c599392c8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7bfb01f3a6b0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_b2aaa\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_b2aaa_level0_col0\" class=\"col_heading level0 col0\" >Metric</th>\n",
              "      <th id=\"T_b2aaa_level0_col1\" class=\"col_heading level0 col1\" >EOA_to_EOA_Mean</th>\n",
              "      <th id=\"T_b2aaa_level0_col2\" class=\"col_heading level0 col2\" >EOA_to_EOA_Std</th>\n",
              "      <th id=\"T_b2aaa_level0_col3\" class=\"col_heading level0 col3\" >Other_EOA_Mean</th>\n",
              "      <th id=\"T_b2aaa_level0_col4\" class=\"col_heading level0 col4\" >Other_EOA_Std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_b2aaa_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_b2aaa_row0_col0\" class=\"data row0 col0\" >eth_value</td>\n",
              "      <td id=\"T_b2aaa_row0_col1\" class=\"data row0 col1\" >0.25</td>\n",
              "      <td id=\"T_b2aaa_row0_col2\" class=\"data row0 col2\" >2.5</td>\n",
              "      <td id=\"T_b2aaa_row0_col3\" class=\"data row0 col3\" >0.25</td>\n",
              "      <td id=\"T_b2aaa_row0_col4\" class=\"data row0 col4\" >2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_b2aaa_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_b2aaa_row1_col0\" class=\"data row1 col0\" >tx_fee</td>\n",
              "      <td id=\"T_b2aaa_row1_col1\" class=\"data row1 col1\" >0.0055</td>\n",
              "      <td id=\"T_b2aaa_row1_col2\" class=\"data row1 col2\" >0.01</td>\n",
              "      <td id=\"T_b2aaa_row1_col3\" class=\"data row1 col3\" >0.0053</td>\n",
              "      <td id=\"T_b2aaa_row1_col4\" class=\"data row1 col4\" >0.031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_b2aaa_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_b2aaa_row2_col0\" class=\"data row2 col0\" >gas_limit</td>\n",
              "      <td id=\"T_b2aaa_row2_col1\" class=\"data row2 col1\" >2.3e+05</td>\n",
              "      <td id=\"T_b2aaa_row2_col2\" class=\"data row2 col2\" >6.9e+05</td>\n",
              "      <td id=\"T_b2aaa_row2_col3\" class=\"data row2 col3\" >1.7e+05</td>\n",
              "      <td id=\"T_b2aaa_row2_col4\" class=\"data row2 col4\" >2.7e+05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_b2aaa_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_b2aaa_row3_col0\" class=\"data row3 col0\" >gas_used</td>\n",
              "      <td id=\"T_b2aaa_row3_col1\" class=\"data row3 col1\" >1.3e+05</td>\n",
              "      <td id=\"T_b2aaa_row3_col2\" class=\"data row3 col2\" >3.2e+05</td>\n",
              "      <td id=\"T_b2aaa_row3_col3\" class=\"data row3 col3\" >1.1e+05</td>\n",
              "      <td id=\"T_b2aaa_row3_col4\" class=\"data row3 col4\" >1.5e+05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_b2aaa_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_b2aaa_row4_col0\" class=\"data row4 col0\" >__row_index</td>\n",
              "      <td id=\"T_b2aaa_row4_col1\" class=\"data row4 col1\" >2.9e+04</td>\n",
              "      <td id=\"T_b2aaa_row4_col2\" class=\"data row4 col2\" >2.1e+04</td>\n",
              "      <td id=\"T_b2aaa_row4_col3\" class=\"data row4 col3\" >2.7e+04</td>\n",
              "      <td id=\"T_b2aaa_row4_col4\" class=\"data row4 col4\" >2.1e+04</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7bfb01f3b820>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_84be3\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_84be3_level0_col0\" class=\"col_heading level0 col0\" >Centrality_Metric</th>\n",
              "      <th id=\"T_84be3_level0_col1\" class=\"col_heading level0 col1\" >EOA_to_EOA_Mean</th>\n",
              "      <th id=\"T_84be3_level0_col2\" class=\"col_heading level0 col2\" >EOA_to_EOA_Std</th>\n",
              "      <th id=\"T_84be3_level0_col3\" class=\"col_heading level0 col3\" >Other_EOA_Mean</th>\n",
              "      <th id=\"T_84be3_level0_col4\" class=\"col_heading level0 col4\" >Other_EOA_Std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_84be3_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_84be3_row0_col0\" class=\"data row0 col0\" >degree</td>\n",
              "      <td id=\"T_84be3_row0_col1\" class=\"data row0 col1\" >0.0011</td>\n",
              "      <td id=\"T_84be3_row0_col2\" class=\"data row0 col2\" >0.0012</td>\n",
              "      <td id=\"T_84be3_row0_col3\" class=\"data row0 col3\" >0.00045</td>\n",
              "      <td id=\"T_84be3_row0_col4\" class=\"data row0 col4\" >0.00053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_84be3_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_84be3_row1_col0\" class=\"data row1 col0\" >closeness</td>\n",
              "      <td id=\"T_84be3_row1_col1\" class=\"data row1 col1\" >0.00011</td>\n",
              "      <td id=\"T_84be3_row1_col2\" class=\"data row1 col2\" >0.00032</td>\n",
              "      <td id=\"T_84be3_row1_col3\" class=\"data row1 col3\" >6.9e-05</td>\n",
              "      <td id=\"T_84be3_row1_col4\" class=\"data row1 col4\" >0.00026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_84be3_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_84be3_row2_col0\" class=\"data row2 col0\" >betweenness</td>\n",
              "      <td id=\"T_84be3_row2_col1\" class=\"data row2 col1\" >1.6e-07</td>\n",
              "      <td id=\"T_84be3_row2_col2\" class=\"data row2 col2\" >5.7e-07</td>\n",
              "      <td id=\"T_84be3_row2_col3\" class=\"data row2 col3\" >4.5e-08</td>\n",
              "      <td id=\"T_84be3_row2_col4\" class=\"data row2 col4\" >3.5e-07</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Centrality Metrics\n",
        "1. Degree Centrality\n",
        "Degree Centrality measures the number of edges a node has. In the context of Ethereum transactions, it indicates how many different addresses an EOA is interacting with. A higher degree centrality signifies that the EOA is involved in more transactions, either as a sender or a receiver.\n",
        "\n",
        "2. Closeness Centrality\n",
        "Closeness Centrality gauges how close a node is to all other nodes in the network, based on the shortest paths. For every pair of nodes, you find the shortest path between them and then average those lengths. In the Ethereum network, a lower average length means the EOA can reach other addresses through fewer hops, making it more central in the network.\n",
        "\n",
        "3. Betweenness Centrality\n",
        "Betweenness Centrality quantifies how often a node appears on the shortest paths between other nodes. In this context, a higher betweenness centrality indicates that the EOA acts as a kind of \"bridge\" within the network, connecting various parts of the Ethereum ecosystem."
      ],
      "metadata": {
        "id": "gMh_It8JxoUr"
      }
    }
  ]
}